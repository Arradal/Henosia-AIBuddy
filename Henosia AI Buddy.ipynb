{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RWMwCWEVN-bC"},"outputs":[],"source":["%%capture\n","#Libraries\n","!pip install gradio\n","!pip install datasets\n","!pip install openai\n","!pip install fuzzywuzzy python-Levenshtein\n","!pip install scikit-learn\n","!pip install pyspellchecker\n","\n","#Imports\n","import gradio as gr\n","from datasets import load_dataset\n","from openai import OpenAI\n","from google.colab import userdata\n","from fuzzywuzzy import process\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from spellchecker import SpellChecker\n","import json\n","import os"]},{"cell_type":"code","source":["#Retrieving the secret API key\n","apikeykey = userdata.get('OPENAI_API_KEY')\n","#Initializing the OpenAI client\n","client = OpenAI(api_key = apikeykey)\n","GPT_MODEL = \"gpt-4\"\n","\n","snippets_dataset = load_dataset(\"P8IxD/Snippets\")\n","\n","#Initializing a new dataset for saved snippets\n","saved_snippets_dataset = []\n","\n","\n","def get_openai_response_code(prompt):\n","    response = client.chat.completions.create(\n","        model=GPT_MODEL,\n","        messages = [{\"role\": \"system\", \"content\": \"You are a code generator tasked with creating a cohesive code snippet from the 'code' column in the provided dataset based on the user's input. Combine relevant snippets in JSX and Tailwind CSS to form a complete functionality. Ensure the output contains only code (no comments, symbols, or additional text) and is formatted as a single traditional code block, for example: ‘[Code]’.\"},\n","                    {\"role\": \"user\", \"content\": \"Generate code for: \" + prompt}],\n","        max_tokens = 200,\n","        temperature = 0.7\n","    )\n","\n","    return response.choices[0].message.content\n","\n","\n","def get_openai_response_name(prompt):\n","    response = client.chat.completions.create(\n","        model=GPT_MODEL,\n","        messages = [{\"role\": \"system\", \"content\": \"Create a new snippet based on the prompt given by the user utilising the existing snippets found in the provided dataset. ChatGPT’s role is to create a name for the generated snippet, used for web development. The snippet name must take inspiration from the ‘name’ column in the provided dataset and be concise with a maximum length of 3 words. Craft the name with professionalism and clarity, incorporating IT terminology where appropriate.\"},\n","                    {\"role\": \"user\", \"content\": \"Generate name for: \" + prompt}],\n","        max_tokens = 10,\n","        temperature = 0.4\n","    )\n","\n","    return response.choices[0].message.content\n","\n","\n","def get_openai_response_description(prompt):\n","    response = client.chat.completions.create(\n","        model=GPT_MODEL,\n","        messages = [{\"role\": \"system\", \"content\": \"Create a new snippet based on the prompt given by the user utilising the existing snippets found in the provided dataset. ChatGPT’s role is to create a description for the generated snippet, used for web development. The snippet description must take inspiration from the ‘description’ column in the provided dataset and be concise with a maximum length of 20 words. Craft the description with professionalism and clarity, incorporating IT terminology where appropriate.\"},\n","                    {\"role\": \"user\", \"content\": \"Generate description for: \" + prompt}],\n","        max_tokens = 25,\n","        temperature = 0.6\n","    )\n","\n","    return response.choices[0].message.content\n","\n","\n","def correct_spelling(user_input, dataset):\n","  words = set()\n","  for entry in dataset:\n","    for word in entry.split():\n","      words.add(word.lower())\n","  #Checks if the original word is already present in the dataset\n","  if user_input.lower() in words:\n","    return user_input.lower()\n","  #Using Fuzzy matching to find most similar word in the dataset\n","  corrected_word, _ = process.extractOne(user_input, words)\n","\n","  return corrected_word\n","\n","#Functions which combines names, descriptions and codes for GPT4 processing\n","def generate_combined_name(names):\n","  combined_prompt = \"Generate a name for: \" + \" \".join(names)\n","  generated_name = get_openai_response_name(combined_prompt)\n","  return generated_name\n","\n","def generate_combined_description(descriptions):\n","  combined_prompt = \"Generate a description for: \" + \" \".join(descriptions)\n","  generated_description = get_openai_response_description(combined_prompt)\n","  return generated_description\n","\n","def generate_combined_code(codes):\n","  combined_prompt = \"Generate code for: \" + \" \".join(codes)\n","  generated_code = get_openai_response_code(combined_prompt)\n","  return generated_code\n","\n","\n","\n","def get_snippets_info(user_input):\n","    user_input = user_input.lower().split()\n","    #Gets a list of all snippet names and components\n","    snippet_info = [entry['Name'].lower() + ' ' + ' '.join(entry['Description']).lower() for entry in snippets_dataset['train']]\n","    #Corrects spelling of user input\n","    corrected_input = [correct_spelling(word, snippet_info) for word in user_input]\n","    vectorizer = TfidfVectorizer()\n","    relevance_threshold = 0.2\n","\n","    combined_names = []\n","    combined_descriptions = []\n","    combined_codes = []\n","\n","    for word in corrected_input:\n","        snippet_info.append(word)\n","        #Computing TF-IDF matrix\n","        tfidf_matrix = vectorizer.fit_transform(snippet_info)\n","        #Computing cosine similarity between user's input and all snippet descriptions\n","        cosine_similarities = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])\n","        #Finding indices of the relevant snippets\n","        relevant_indices = [i for i, score in enumerate(cosine_similarities[0]) if score > relevance_threshold]\n","\n","        combined_name = \"\"\n","        combined_description = \"\"\n","        combined_code = \"\"\n","\n","        #Iterating through the relevant indices to collect names, descriptions and codes\n","        for index in relevant_indices:\n","            name = snippets_dataset['train'][int(index)]['Name']\n","            description = snippets_dataset['train'][int(index)]['Description']\n","            code = snippets_dataset['train'][int(index)]['Code']\n","            #Concatenating the names, descriptions and codes\n","            combined_name += name + \" \"\n","            combined_description += description + \" \"\n","            combined_code += code + \" \"\n","        #Appends combined names, descriptions and codes to their corresponding lists for \"AI Processing\"\n","        combined_names.append(combined_name.strip())\n","        final_combined_name = generate_combined_name(combined_names)\n","\n","        combined_descriptions.append(combined_description.strip())\n","        final_combined_description = generate_combined_description(combined_descriptions)\n","\n","        combined_codes.append(combined_code.strip())\n","        final_combined_code = generate_combined_code(combined_codes)\n","\n","    return final_combined_name, final_combined_description, final_combined_code\n","\n","\n","def save_snippet(name, description, code):\n","\n","  if \"Could not create a snippet\" in name or \"No snippet to save\" in name or \"Snippet saved successfully\" in name:\n","    return \"No snippet to save\"\n","\n","  if not name.strip() and not description.strip() and not code.strip():\n","    return \"No snippet to save\"\n","  #Create a dictionary for the snippet\n","  snippet = {\n","      'Name': name,\n","      'Descriptions': description,\n","      'Code': code\n","  }\n","  saved_snippets_dataset.append(snippet)\n","  #Checks if the file already exists\n","  if os.path.exists('saved_snippets.json'):\n","    #Load existing data if the file exists\n","    with open('saved_snippets.json', 'r') as f:\n","      data = json.load(f)\n","\n","  else:\n","    #Initialize empty list if file doesn't exist\n","    data = []\n","\n","  data.append(snippet)\n","  #Write data back to the file\n","  with open('saved_snippets.json', 'w') as f:\n","    json.dump(saved_snippets_dataset, f)\n","\n","  return \"Snippet saved successfully!\"\n","\n","#Creating the interface\n","with gr.Blocks() as interface:\n","  gr.Markdown(\"AI Buddy - Enter instructions for the snippet you want to generate.\")\n","  with gr.Row():\n","      user_input = gr.Textbox(label=\"Input\")\n","\n","  with gr.Row():\n","      submit_button = gr.Button(\"Submit\")\n","      save_button = gr.Button(\"Save\")\n","\n","  with gr.Row():\n","    with gr.Column(scale=0.5, min_width=100):\n","      name_output = gr.Textbox(label=\"Name\")\n","      descriptions_output = gr.Textbox(label=\"Description\")\n","    code_output = gr.Code(label=\"Code\")\n","\n","  submit_button.click(get_snippets_info, inputs=user_input, outputs=[name_output, descriptions_output, code_output])\n","  save_button.click(save_snippet, inputs=[name_output, descriptions_output, code_output], outputs=name_output)\n","\n","#Launch the interface\n","interface.launch()"],"metadata":{"id":"Z9LAnY-4I-Oc","colab":{"base_uri":"https://localhost:8080/","height":684},"executionInfo":{"status":"ok","timestamp":1715763337745,"user_tz":-120,"elapsed":5144,"user":{"displayName":"Maja Thestesen","userId":"02145327741846882853"}},"outputId":"2763296d-00ae-4a2c-d98e-40d92e98fc77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio/layouts/column.py:53: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://c20334775ef6cee82f.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://c20334775ef6cee82f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":15}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}